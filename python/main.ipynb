{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8335d8e9",
   "metadata": {},
   "source": [
    "# 引入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae94fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:17.635263Z",
     "start_time": "2024-12-12T08:53:15.696409Z"
    }
   },
   "outputs": [],
   "source": [
    "import bisect\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "from itertools import cycle\n",
    "from typing import Tuple, Union, List, Dict\n",
    "import random as sys_random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.font_manager import FontManager\n",
    "# matplotlib.use('Qt5Agg')\n",
    "fm = FontManager()\n",
    "mat_fonts = set(f.name for f in fm.ttflist)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from pywayne.tools import wayne_print\n",
    "from pywayne.dsp import butter_bandpass_filter\n",
    "\n",
    "import neptune\n",
    "from neptune.integrations.python_logger import NeptuneHandler\n",
    "from neptune.types import File\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('micro-hand-gesture-logger')\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "wayne_print(f'{torch.__version__=}', 'yellow')\n",
    "wayne_print(f'{torch.cuda.is_available()}', 'green')\n",
    "wayne_print(f'{torch.backends.mps.is_available()}', 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf61ff",
   "metadata": {},
   "source": [
    "# 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e5202c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:17.638778Z",
     "start_time": "2024-12-12T08:53:17.636575Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED=1\n",
    "\n",
    "def set_torch_seed(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    sys_random.seed(seed)  # Python random module.\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73c4c1",
   "metadata": {},
   "source": [
    "# 数据集描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc7329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:17.710993Z",
     "start_time": "2024-12-12T08:53:17.639555Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('aw10_data.h5', 'r') as h5:\n",
    "    for scene in sorted(h5.keys()):\n",
    "        attr = h5[scene].attrs\n",
    "        wayne_print(f\"{scene=}, {len(h5[scene])=}, {attr['date']=}, {attr['force_level']=}, {attr['handness']=}, {attr['note']=}, {attr['scene_kw']=}, {attr['scene_property']=}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81bec4d",
   "metadata": {},
   "source": [
    "# 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8c61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:21.097761Z",
     "start_time": "2024-12-12T08:53:21.042603Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "with h5py.File('aw10_data.h5', 'r+') as h5:\n",
    "    for scene_id, scene in enumerate(sorted(h5.keys())):\n",
    "        for idx in h5[scene].keys():\n",
    "            wayne_print(f'{scene=}, {idx=}')\n",
    "            kw = f'{scene}/{idx}'\n",
    "            acc = h5[kw]['acc_rawdata']\n",
    "            acc_filtered = butter_bandpass_filter(\n",
    "                np.array(acc) / 9.81, order=2, lo=0.1, hi=40, fs=100.0, btype='bandpass', realtime=False\n",
    "            )\n",
    "            gyro = h5[kw]['gyro_rawdata']\n",
    "            gyro_filtered = gyro\n",
    "            \n",
    "            if scene_id == 0 and idx == '0':\n",
    "                fig, ax = plt.subplots(2, 2, sharex='all')\n",
    "                ax[0][0].plot(acc)\n",
    "                ax[1][0].plot(acc_filtered)\n",
    "                ax[0][1].plot(gyro)\n",
    "                ax[1][1].plot(gyro_filtered)\n",
    "                plt.show()\n",
    "            \n",
    "            if 'acc_filtered' in h5[kw]:\n",
    "                del h5[kw]['acc_filtered']\n",
    "            if 'gyro_filtered' in h5[kw]:\n",
    "                del h5[kw]['gyro_filtered']\n",
    "\n",
    "            h5[kw]['acc_filtered'] = acc_filtered\n",
    "            h5[kw]['gyro_filtered'] = gyro_filtered\n",
    "            print(acc.shape, gyro.shape, acc_filtered.shape, gyro_filtered.shape)\n",
    "            del acc, gyro, acc_filtered, gyro_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2670a",
   "metadata": {},
   "source": [
    "# 数据集划分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88016871",
   "metadata": {},
   "source": [
    "## 方法一、Intra-individual Stability\n",
    "最理想情况，单条数据分成60% / 20% / 20%的训练集、验证集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc785e28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:24.002500Z",
     "start_time": "2024-12-12T08:53:23.953270Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_labels = ('train', 'valid', 'test')\n",
    "for dataset_label in dataset_labels:\n",
    "    file_name = f'{dataset_label}.h5'\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "\n",
    "h5 = h5py.File('aw10_data.h5', 'r')\n",
    "h5s = {label: h5py.File(f'{label}.h5', 'a') for label in dataset_labels}\n",
    "for scene in sorted(h5.keys()):\n",
    "    attr = h5[scene].attrs\n",
    "#     wayne_print(f\"{scene=}, {len(h5[scene])=}, {attr['date']=}, {attr['force_level']=}, {attr['handness']=}, {attr['note']=}, {attr['scene_kw']=}, {attr['scene_property']=}\\n\")\n",
    "    n_segments = len(h5[scene])\n",
    "    all_idx = list(map(str, range(n_segments)))\n",
    "    random.shuffle(all_idx)\n",
    "    training_size = int(0.6 * n_segments)\n",
    "    validation_size = int(0.2 * n_segments)\n",
    "    test_size = n_segments - training_size - validation_size\n",
    "    training_indices = all_idx[:training_size]\n",
    "    validation_indices = all_idx[training_size:training_size + validation_size]\n",
    "    test_indices = all_idx[-test_size:]\n",
    "#     print(all_idx)\n",
    "    for dataset_label, data_indices in zip(\n",
    "        dataset_labels,\n",
    "        (training_indices, validation_indices, test_indices)\n",
    "    ):\n",
    "        for i, idx in enumerate(data_indices):\n",
    "#             print(f'{scene}/{idx}/acc_rawdata')\n",
    "            h5s[dataset_label][f'{scene}/{i}/acc_rawdata'] = np.array(h5[scene][idx]['acc_rawdata'])\n",
    "            h5s[dataset_label][f'{scene}/{i}/gyro_rawdata'] = np.array(h5[scene][idx]['gyro_rawdata'])\n",
    "            h5s[dataset_label][f'{scene}/{i}/acc_filtered'] = np.array(h5[scene][idx]['acc_filtered'])\n",
    "            h5s[dataset_label][f'{scene}/{i}/gyro_filtered'] = np.array(h5[scene][idx]['gyro_filtered'])\n",
    "        \n",
    "        for attr_k, attr_v in h5[scene].attrs.items():\n",
    "            h5s[dataset_label][scene].attrs[attr_k] = attr_v\n",
    "\n",
    "h5.close()\n",
    "[h.close() for h in h5s.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745df10",
   "metadata": {},
   "source": [
    "## 方法二、Cross-subject Transferability\n",
    "1. 80%的人（个体）用于训练，20%的人（个体）用于测试，注意80%的人的数据量为全部数据的60%\n",
    "2. 在线学习，先让用户打开app后采集每个场景的数组1～3次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72151ede",
   "metadata": {},
   "source": [
    "# 构建数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c584495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:26.235206Z",
     "start_time": "2024-12-12T08:53:26.162475Z"
    }
   },
   "outputs": [],
   "source": [
    "y_idx2lbl = ['单击', '双击', '握拳', '左滑', '右滑', '鼓掌', '抖腕', '拍打', '日常']\n",
    "y_lbl2idx = {l: i for i, l in enumerate(y_idx2lbl)}\n",
    "y_lbl2onehot = {l: np.eye(len(y_idx2lbl))[i] for i, l in enumerate(y_idx2lbl)}\n",
    "wayne_print(y_lbl2idx, 'green')\n",
    "wayne_print(y_lbl2onehot, 'green')\n",
    "\n",
    "class MHGDataSet(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        self.h5_path = h5_path\n",
    "        self.prefix_sum = [0]\n",
    "        self.scenes = []\n",
    "        self.init()\n",
    "        self._all_data = None\n",
    "\n",
    "    def init(self):\n",
    "        with h5py.File(self.h5_path, 'r') as h5:\n",
    "            for scene in sorted(h5.keys()):\n",
    "                attr = h5[scene].attrs\n",
    "#                 print(attr.keys())\n",
    "                scene_len = len(h5[scene])\n",
    "                self.prefix_sum.append(self.prefix_sum[-1] + scene_len)\n",
    "                self.scenes.append(scene)\n",
    "#                 wayne_print(f\"{scene=}, {len(h5[scene])=}, {attr['date']=}, {attr['force_level']=}, {attr['handness']=}, {attr['note']=}, {attr['scene_kw']=}, {attr['scene_property']=}\\n\")\n",
    "        print(self.prefix_sum)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.prefix_sum[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        which_scene_idx, which_scene = self.get_scene_by_idx(idx)\n",
    "        offset = str(idx - self.prefix_sum[which_scene_idx])\n",
    "#         print(f'{offset=}')\n",
    "        \n",
    "        with h5py.File(self.h5_path, 'r') as h5:\n",
    "            data = h5[which_scene][offset]  # 获取具体数据\n",
    "            attr = dict(h5[which_scene].attrs)  # 转换属性为字典\n",
    "#             print(which_scene)\n",
    "#             print(22,h5[which_scene].attrs.keys())\n",
    "            acc_rawdata = np.array(data['acc_rawdata'], dtype=float)\n",
    "            gyro_rawdata = np.array(data['gyro_rawdata'], dtype=float)\n",
    "            acc_filtered = np.array(data['acc_filtered'], dtype=float)\n",
    "            gyro_filtered = np.array(data['gyro_filtered'], dtype=float)\n",
    "#             x = np.c_[acc_rawdata, gyro_rawdata]\n",
    "            x = torch.from_numpy(np.c_[acc_filtered, gyro_filtered])\n",
    "            x = x.permute(1, 0)\n",
    "            y = torch.from_numpy(y_lbl2onehot[attr['scene_kw']])\n",
    "            return x.float(), y.float()\n",
    "        \n",
    "    def get_all_data(self):\n",
    "        dtype = torch.float32\n",
    "        x_all, y_all = [], []\n",
    "        with h5py.File(self.h5_path, 'r') as h5:\n",
    "            for scene in sorted(h5.keys()):\n",
    "                for idx in h5[scene]:\n",
    "                    kw = f'{scene}/{idx}'\n",
    "                    x = torch.tensor(np.c_[\n",
    "                        h5[kw]['acc_filtered'][()],\n",
    "                        h5[kw]['gyro_filtered'][()]\n",
    "                    ], dtype=dtype)\n",
    "                    y = torch.tensor(y_lbl2onehot[h5[scene].attrs['scene_kw']], \n",
    "                                   dtype=dtype)\n",
    "                    x_all.append(x)\n",
    "                    y_all.append(y)\n",
    "                    del x, y\n",
    "\n",
    "        x_all_tensor = torch.stack(x_all).permute(0, 2, 1)\n",
    "        y_all_tensor = torch.stack(y_all)\n",
    "        print(x_all_tensor.shape, y_all_tensor.shape)\n",
    "        del x_all, y_all\n",
    "        return x_all_tensor, y_all_tensor\n",
    "        \n",
    "    def get_scene_by_idx(self, idx):\n",
    "        which_scene_idx = bisect.bisect_right(self.prefix_sum, idx) - 1\n",
    "        which_scene = self.scenes[which_scene_idx]\n",
    "        return which_scene_idx, which_scene\n",
    "    \n",
    "    def get_attr_by_idx(self, idx):\n",
    "        which_scene_idx, which_scene = self.get_scene_by_idx(idx)\n",
    "        offset = str(idx - self.prefix_sum[which_scene_idx])\n",
    "        \n",
    "        with h5py.File(self.h5_path, 'r') as h5:\n",
    "            data = h5[which_scene][offset]  # 获取具体数据\n",
    "            attr = dict(h5[which_scene].attrs)  # 转换属性为字典\n",
    "            return attr\n",
    "        \n",
    "    def visualize_by_idx(self, idx):\n",
    "        plt.close('all')\n",
    "        x, y = self[idx]\n",
    "        attr = self.get_attr_by_idx(idx)\n",
    "        fig, ax = plt.subplots(2, 1, sharex='all')\n",
    "        ax[0].plot(x.T[:,:3])\n",
    "        ax[0].legend(('x','y','z'))\n",
    "        ax[1].plot(x.T[:,3:])\n",
    "        ax[1].legend(('x','y','z'))\n",
    "        [a.grid(True) for a in ax]\n",
    "        plt.suptitle('_'.join(attr.values()))\n",
    "        plt.show()\n",
    "\n",
    "dataset = MHGDataSet('train.h5')\n",
    "print(len(dataset))\n",
    "x, y = dataset[400]\n",
    "print(x.shape, y)\n",
    "print(dataset.get_scene_by_idx(400))\n",
    "print(dataset.get_attr_by_idx(400))\n",
    "dataset.visualize_by_idx(400)\n",
    "\n",
    "train_dataset = MHGDataSet('train.h5')\n",
    "valid_dataset = MHGDataSet('valid.h5')\n",
    "test_dataset = MHGDataSet('test.h5')\n",
    "total_dataset = MHGDataSet('aw10_data.h5')\n",
    "\n",
    "x, y = total_dataset.get_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b26fa7",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293ee31",
   "metadata": {},
   "source": [
    "## baseline： vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f787388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T08:53:28.310785Z",
     "start_time": "2024-12-12T08:53:28.279891Z"
    }
   },
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "epoch_num = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "dl_params = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'optimizer': 'Adam',\n",
    "    'train_batch_size': train_batch_size,\n",
    "    'valid_batch_size': valid_batch_size,\n",
    "    'epoch_num': epoch_num,\n",
    "    'metrics': 'CrossEntropy'\n",
    "}\n",
    "\n",
    "class VanillaCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VanillaCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(6, 12, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm1d(12)\n",
    "        self.maxpool1 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(12, 12, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(12)\n",
    "        self.maxpool2 = nn.MaxPool1d(4, stride=4)\n",
    "        self.conv3 = nn.Conv1d(12, 6, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm1d(6)\n",
    "#         self.maxpool3 = nn.MaxPool1d(4, stride=4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(6 * 7, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xx = self.maxpool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        xx = self.maxpool2(self.relu(self.bn2(self.conv2(xx))))\n",
    "        xx = self.relu(self.bn3(self.conv3(xx)))\n",
    "        \n",
    "#         print(xx.shape)\n",
    "        xx = xx.view(xx.size(0), -1)  # flatten\n",
    "#         print(xx.shape)\n",
    "        xx = self.fc(xx)  # logits\n",
    "        \n",
    "        return xx # nn.Softmax(xx)\n",
    "    \n",
    "def count_parameters(model):\n",
    "    ret = sum(p.numel() for p in model.parameters())\n",
    "    wayne_print(f'{ret=}', 'yellow')\n",
    "    return ret\n",
    "\n",
    "def weight_init():\n",
    "    for m in model.modules():\n",
    "        for name, param in m.named_parameters():\n",
    "            if type(m) in (nn.GRU, nn.LSTM, nn.RNN):\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.orthogonal_(param.data)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "            else:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                    nn.initt.constant_(m.bias, 0)\n",
    "                elif type(m) in (nn.Conv1d, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                elif typee(m) in (nn.BatchNorm1d, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    \n",
    "model = VanillaCNN(len(y_idx2lbl)).to(device)\n",
    "print(len(y_idx2lbl))\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf28fa",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9b0be",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-12T08:53:33.043Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.txt', delimiter=',')\n",
    "data[:,:3] = butter_bandpass_filter(\n",
    "    data[:,:3] / 9.81,  # 转换为g\n",
    "    order=2,\n",
    "    lo=0.1,\n",
    "    hi=40,\n",
    "    fs=100.0,\n",
    "    btype='bandpass',\n",
    "    realtime=False\n",
    ")\n",
    "# print(data.shape)\n",
    "# plt.close('all')\n",
    "# plt.plot(data[:,:3])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 转换为torch tensor并调整维度\n",
    "x = torch.from_numpy(data.T).float().unsqueeze(0)  # [1, 6, 60]\n",
    "print(x.shape)\n",
    "x = x.to('cpu')\n",
    "\n",
    "# 模型预测\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "#     probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "#     predicted_class = torch.argmax(output, dim=1).item()\n",
    "#     confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "# # 获取预测结果\n",
    "# class_names = ['单击', '双击', '握拳', '左滑', '右滑', '鼓掌', '抖腕', '拍打', '日常']\n",
    "# predicted_label = class_names[predicted_class]\n",
    "\n",
    "# print(f\"Predicted gesture: {predicted_label} (confidence: {confidence:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa662568",
   "metadata": {},
   "source": [
    "# 定义DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727ef503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:03:15.340971Z",
     "start_time": "2024-12-05T16:03:15.339066Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5964a",
   "metadata": {},
   "source": [
    "# 配置Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f484d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:03:15.345984Z",
     "start_time": "2024-12-05T16:03:15.343488Z"
    }
   },
   "outputs": [],
   "source": [
    "use_neptune = True\n",
    "\n",
    "if use_neptune:\n",
    "    run = neptune.init_run(\n",
    "        project=\"wangyendt/Micro-Hand-Gesture\",\n",
    "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmYWQwNWMyNC1hYzY5LTRhZTEtOGJiYS1lZjdkNGE1NjRiY2UifQ==\",\n",
    "        source_files=[\"**/*.py\", \"**/*.ipynb\", \"config.yaml\"],\n",
    "        description='Micro Hand Gesture EDA',\n",
    "        tags=['vanilla CNN', 'v0.0.1']\n",
    "    )  # your credentials\n",
    "    run[\"parameters\"] = dl_params\n",
    "    # Add additional parameters\n",
    "    set_torch_seed()\n",
    "    run['parameters/seed'] = RANDOM_SEED\n",
    "    run['parameters/num_of_params'] = count_parameters(model)\n",
    "\n",
    "    logger.addHandler(NeptuneHandler(run=run))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0f11e",
   "metadata": {},
   "source": [
    "# 定义混淆矩阵函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa6d7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:03:15.350357Z",
     "start_time": "2024-12-05T16:03:15.346975Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_pred, y_true, class_mapping, dataset_name):\n",
    "    # 确保输入是 numpy 数组\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # 获取预测值和真实值中的类别\n",
    "    classes = sorted(list(set(np.unique(y_pred)) | set(np.unique(y_true))))\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 获取映射后的标签\n",
    "    labels = [class_mapping[i] for i in classes]\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "    # 创建图形\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 创建主热力图，但不显示上方标签\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                     xticklabels=[], yticklabels=labels)\n",
    "    \n",
    "    # 创建顶部的第二个x轴\n",
    "    ax2 = ax.twiny()\n",
    "    \n",
    "    # 设置上方x轴的刻度和标签\n",
    "    ax2.set_xlim(ax.get_xlim())\n",
    "    ax2.set_xticks(np.arange(len(labels)) + 0.5)\n",
    "    ax2.set_xticklabels(labels, ha='left')\n",
    "    \n",
    "    # 设置标签位置\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.xaxis.set_label_position('top')\n",
    "    \n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix of {dataset_name}')\n",
    "    \n",
    "    # 调整布局以防止标签被裁剪\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return cm, classes, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6a447",
   "metadata": {},
   "source": [
    "# 根据混淆矩阵计算各种metric函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db5c791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:03:15.363281Z",
     "start_time": "2024-12-05T16:03:15.351481Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_advanced_metrics(confusion_matrix: np.ndarray, \n",
    "                             y_true: np.ndarray = None,\n",
    "                             y_pred: np.ndarray = None,\n",
    "                             y_prob: np.ndarray = None) -> dict:\n",
    "    \"\"\"\n",
    "    计算混淆矩阵的各种高级评估指标，并可选择性地生成ROC曲线。\n",
    "    \n",
    "    参数:\n",
    "        confusion_matrix: numpy.ndarray, 形状为(n, n)的混淆矩阵\n",
    "        y_true: 真实标签，形状为(N,)\n",
    "        y_pred: 预测的类别，形状为(N,)\n",
    "        y_prob: 预测的概率分布，形状为(N, n_classes)，用于ROC曲线\n",
    "    \"\"\"\n",
    "    if not isinstance(confusion_matrix, np.ndarray):\n",
    "        confusion_matrix = np.array(confusion_matrix)\n",
    "    \n",
    "    n_classes = confusion_matrix.shape[0]\n",
    "    \n",
    "    # 1. 基础指标计算\n",
    "    tp = np.diag(confusion_matrix)\n",
    "    fp = np.sum(confusion_matrix, axis=0) - tp\n",
    "    fn = np.sum(confusion_matrix, axis=1) - tp\n",
    "    tn = np.sum(confusion_matrix) - (tp + fp + fn)\n",
    "    \n",
    "    # 2. 计算基础指标\n",
    "    precision = np.zeros(n_classes)\n",
    "    recall = np.zeros(n_classes)\n",
    "    specificity = np.zeros(n_classes)\n",
    "    f1_score = np.zeros(n_classes)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i] = tp[i] / (tp[i] + fp[i]) if (tp[i] + fp[i]) > 0 else 0\n",
    "        recall[i] = tp[i] / (tp[i] + fn[i]) if (tp[i] + fn[i]) > 0 else 0\n",
    "        specificity[i] = tn[i] / (tn[i] + fp[i]) if (tn[i] + fp[i]) > 0 else 0\n",
    "        f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0\n",
    "    \n",
    "    # 3. 计算高级指标\n",
    "    # Cohen's Kappa\n",
    "    total = np.sum(confusion_matrix)\n",
    "    observed_accuracy = np.sum(tp) / total\n",
    "    expected_accuracy = sum(np.sum(confusion_matrix, axis=0) * np.sum(confusion_matrix, axis=1)) / (total * total)\n",
    "    kappa = (observed_accuracy - expected_accuracy) / (1 - expected_accuracy)\n",
    "    \n",
    "    # Balanced Accuracy\n",
    "    balanced_accuracy = np.mean(recall)\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC)\n",
    "    def multiclass_mcc(confusion_matrix):\n",
    "        t_sum = confusion_matrix.sum()\n",
    "        s = (confusion_matrix / t_sum).sum()\n",
    "        r = np.sum(confusion_matrix, axis=1)\n",
    "        c = np.sum(confusion_matrix, axis=0)\n",
    "        t = np.trace(confusion_matrix)\n",
    "        n = t_sum * t - np.sum(r * c)\n",
    "        d = np.sqrt((t_sum**2 - np.sum(c * c)) * (t_sum**2 - np.sum(r * r)))\n",
    "        return n / d if d != 0 else 0\n",
    "    \n",
    "    mcc = multiclass_mcc(confusion_matrix)\n",
    "    \n",
    "    # 4. 如果提供了预测概率，计算ROC曲线相关指标\n",
    "    roc_data = None\n",
    "    if y_true is not None and y_prob is not None:\n",
    "        roc_data = calculate_multiclass_roc(y_true, y_prob, n_classes)\n",
    "    \n",
    "    return {\n",
    "        'basic_metrics': {\n",
    "            'precision': {\n",
    "                'per_class': precision.tolist(),\n",
    "                'macro_avg': float(np.mean(precision))\n",
    "            },\n",
    "            'recall': {\n",
    "                'per_class': recall.tolist(),\n",
    "                'macro_avg': float(np.mean(recall))\n",
    "            },\n",
    "            'f1_score': {\n",
    "                'per_class': f1_score.tolist(),\n",
    "                'macro_avg': float(np.mean(f1_score))\n",
    "            },\n",
    "            'accuracy': float(observed_accuracy)\n",
    "        },\n",
    "        'advanced_metrics': {\n",
    "            'specificity': {\n",
    "                'per_class': specificity.tolist(),\n",
    "                'macro_avg': float(np.mean(specificity))\n",
    "            },\n",
    "            'balanced_accuracy': float(balanced_accuracy),\n",
    "            'cohen_kappa': float(kappa),\n",
    "            'matthews_correlation_coefficient': float(mcc)\n",
    "        },\n",
    "        'roc_data': roc_data\n",
    "    }\n",
    "\n",
    "def calculate_multiclass_roc(y_true: np.ndarray, y_prob: np.ndarray, n_classes: int) -> Dict:\n",
    "    \"\"\"\n",
    "    计算多分类ROC曲线（one-vs-rest方式）\n",
    "    \n",
    "    参数:\n",
    "        y_true: 真实标签，形状为(N,)\n",
    "        y_prob: 预测概率，形状为(N, n_classes)\n",
    "        n_classes: 类别数量\n",
    "    \"\"\"\n",
    "    # 将标签进行二值化处理\n",
    "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    \n",
    "    # 计算每个类别的ROC曲线和AUC\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # 计算微平均ROC曲线\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    return {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "def plot_roc_curves(roc_data: Dict, n_classes: int, class_names: List[str] = None):\n",
    "    \"\"\"\n",
    "    绘制ROC曲线\n",
    "    \n",
    "    参数:\n",
    "        roc_data: ROC曲线数据\n",
    "        n_classes: 类别数量\n",
    "        class_names: 类别名称列表（可选）\n",
    "    \"\"\"\n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 设置颜色循环\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray'])\n",
    "    \n",
    "    # 绘制每个类别的ROC曲线\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        class_label = f'类别 {i}' if class_names is None else class_names[i]\n",
    "        plt.plot(roc_data['fpr'][i], roc_data['tpr'][i], color=color, lw=2,\n",
    "                label=f'ROC曲线 {class_label} (AUC = {roc_data[\"roc_auc\"][i]:0.2f})')\n",
    "    \n",
    "    # 绘制微平均ROC曲线\n",
    "    plt.plot(roc_data['fpr']['micro'], roc_data['tpr']['micro'],\n",
    "            label=f'微平均ROC曲线 (AUC = {roc_data[\"roc_auc\"][\"micro\"]:0.2f})',\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "    \n",
    "    # 绘制对角线\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('假阳性率')\n",
    "    plt.ylabel('真阳性率')\n",
    "    plt.title('多分类ROC曲线 (One-vs-Rest)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35e46d",
   "metadata": {},
   "source": [
    "# 模型训练、模型验证、模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2fce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:03:15.366843Z",
     "start_time": "2024-12-05T16:03:15.364373Z"
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "print(f\"当前内存使用: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d8260",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-05T16:03:11.755Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "use_wandb = False\n",
    "\n",
    "best_model_accuracy = collections.defaultdict(float)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    running_train_loss = []\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    for i, loader in enumerate(train_loader):\n",
    "        x, y = loader\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "#         print(type(x), type(y))\n",
    "#         print(x.shape, y.shape)\n",
    "        y_out = model(x)\n",
    "        loss = criterion(y_out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        log = f'epoch={epoch+1},i={i+1}/{len(train_loader)},training loss={train_loss}'\n",
    "        sys.stdout.write(\"\\r{0}\".format(log))\n",
    "        sys.stdout.flush()\n",
    "        running_train_loss.append(train_loss)\n",
    "    train_losses.append(np.mean(running_train_loss))\n",
    "    print()\n",
    "    print(f'epoch={epoch+1}, training loss={train_losses[-1]:.5f}')\n",
    "    \n",
    "    running_valid_loss = []\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        vsets = train_dataset, valid_dataset, total_dataset\n",
    "        vset_names = ('train', 'valid', 'total')\n",
    "    else:\n",
    "        vsets = train_dataset, valid_dataset\n",
    "        vset_names = ('train', 'valid')\n",
    "    for vset, vset_name in zip(vsets, vset_names):\n",
    "        vx, vy = vset.get_all_data()\n",
    "        vx = vx.to(device)\n",
    "        vy_out = model(vx).cpu()\n",
    "        vloss = criterion(vy_out, vy)\n",
    "        log = f'epoch={epoch+1}, vset loss={vloss.item()}'\n",
    "        sys.stdout.write(\"\\r{0}\".format(log))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        vy_prob = torch.nn.functional.softmax(vy_out, dim=1).numpy()\n",
    "        vy_pred = np.argmax(vy_out.numpy(), axis=1)\n",
    "        vy_gt = np.argmax(vy.numpy(), axis=1)\n",
    "        cm, classes, cm_fig = calculate_confusion_matrix(vy_pred, vy_gt, y_idx2lbl, vset_name)\n",
    "        \n",
    "        print(cm.shape, vy_gt.shape, vy_pred.shape)\n",
    "        # 计算指标\n",
    "        metrics = calculate_advanced_metrics(\n",
    "            cm,\n",
    "            y_true=vy_gt,\n",
    "            y_pred=vy_pred,\n",
    "            y_prob=vy_prob\n",
    "        )\n",
    "        # 绘制ROC曲线\n",
    "        if metrics['roc_data']:\n",
    "            roc_fig = plot_roc_curves(metrics['roc_data'], cm.shape[1])\n",
    "        \n",
    "        accuracy = metrics['basic_metrics']['accuracy']\n",
    "        if accuracy > best_model_accuracy[vset_name]:\n",
    "            best_model_accuracy[vset_name] = accuracy\n",
    "            best_model_path = f'{vset_name}_epoch={epoch}_accuracy={accuracy:.3f}.pt'\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                best_model_path\n",
    "            )\n",
    "            if use_neptune:\n",
    "                run[f'model/saved_model/{vset_name}_epoch={epoch}_accuracy={accuracy:.3f}'].upload(best_model_path)\n",
    "                run[f'{vset_name}/best_accuracy'].log(accuracy)\n",
    "                run[f'{vset_name}/best_accuracy_figs/epoch_{epoch+1}_confusion_matrix'].upload(cm_fig)\n",
    "                run[f'{vset_name}/best_accuracy_figs/epoch_{epoch+1}_roc'].upload(roc_fig)\n",
    "        \n",
    "\n",
    "        # 打印结果\n",
    "#         print(\"\\n基础指标:\")\n",
    "#         print(f\"准确率 (Accuracy): {metrics['basic_metrics']['accuracy']:.3f}\")\n",
    "#         print(f\"宏平均精确率: {metrics['basic_metrics']['precision']['macro_avg']:.3f}\")\n",
    "#         print(f\"宏平均召回率: {metrics['basic_metrics']['recall']['macro_avg']:.3f}\")\n",
    "#         print(f\"宏平均F1分数: {metrics['basic_metrics']['f1_score']['macro_avg']:.3f}\")\n",
    "\n",
    "#         print(\"\\n高级指标:\")\n",
    "#         print(f\"Cohen's Kappa: {metrics['advanced_metrics']['cohen_kappa']:.3f}\")\n",
    "#         print(f\"Matthews相关系数: {metrics['advanced_metrics']['matthews_correlation_coefficient']:.3f}\")\n",
    "#         print(f\"平衡准确率: {metrics['advanced_metrics']['balanced_accuracy']:.3f}\")\n",
    "\n",
    "        \n",
    "        if use_neptune:\n",
    "            run[f'{vset_name}/loss'].log(train_losses[-1])\n",
    "            run[f'{vset_name}/figs/epoch_{epoch+1}_confusion_matrix'].upload(cm_fig)\n",
    "            run[f'{vset_name}/accuracy'].log(metrics['basic_metrics']['accuracy'])\n",
    "            run[f'{vset_name}/precision'].log(metrics['basic_metrics']['precision']['macro_avg'])\n",
    "            run[f'{vset_name}/recall'].log(metrics['basic_metrics']['recall']['macro_avg'])\n",
    "            run[f'{vset_name}/f1_score'].log(metrics['basic_metrics']['f1_score']['macro_avg'])\n",
    "            run[f'{vset_name}/cohen_kappa'].log(metrics['advanced_metrics']['cohen_kappa'])\n",
    "            run[f'{vset_name}/matthews_correlation_coefficient'].log(metrics['advanced_metrics']['matthews_correlation_coefficient'])\n",
    "            run[f'{vset_name}/balanced_accuracy'].log(metrics['advanced_metrics']['balanced_accuracy'])\n",
    "            run[f'{vset_name}/figs/epoch_{epoch+1}_roc'].upload(roc_fig)\n",
    "            \n",
    "    if use_neptune:\n",
    "        run[f'epoch'] = epoch\n",
    "\n",
    "if use_neptune:\n",
    "    run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20426509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:48:19.638938Z",
     "start_time": "2024-12-05T15:48:19.638932Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68625a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
